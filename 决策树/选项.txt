
- 划分规则（SplitCriterion）是用于选择最优分裂属性和分裂点的准则，它可以衡量分裂后的子节点的纯度或不确定性。fitctree函数支持以下几种划分规则：
    - 'gdi'：基尼不纯度（Gini diversity index），它是指每个类别在节点中出现的概率与其他类别出现的概率的乘积之和，越小越好。
    - 'deviance'：偏差（deviance），它是指每个类别在节点中出现的概率与其理论概率（根据先验分布）的差异之和，越小越好。
    - 'twoing'：两分法（twoing rule），它是指将节点中的类别分成两组，然后计算两组之间的差异，越大越好。
- 交叉验证方法（CrossVal）是用于评估模型泛化能力的统计方法，它可以帮助我们选择最优的模型和参数，避免过拟合或欠拟合的问题。fitctree函数支持以下几种交叉验证方法：
    - 'off'：不进行交叉验证，只使用训练数据拟合模型。
    - 'on'：进行10折交叉验证，即将训练数据随机分成10份，每次用9份作为训练集，1份作为测试集，重复10次，然后取平均误差作为交叉验证误差。
    - 'Holdout'：进行留出法交叉验证，即将训练数据按照一定比例分成训练集和测试集，只用一次，然后计算测试误差作为交叉验证误差。
    - 'Leaveout'：进行留一法交叉验证，即每次只留下一个数据作为测试集，其余的数据作为训练集，重复n次（n为数据集大小），然后取平均误差作为交叉验证误差。
    - 'KFold'：进行k折交叉验证，即将训练数据随机分成k份，每次用k-1份作为训练集，1份作为测试集，重复k次，然后取平均误差作为交叉验证误差。
    - 'CVPartition'：使用自定义的交叉验证划分方案，可以使用cvpartition函数来创建一个cvpartition对象，并传入fitctree函数中。
- 最小叶结点数（MinLeafSize）是用于控制决策树复杂度和防止过拟合的参数，它指定了每个叶结点中必须包含的最少观测数。如果一个节点中的观测数小于这个值，则不再进行分裂。最小叶结点数越大，则决策树越简单；最小叶结点数越小，则决策树越复杂。
